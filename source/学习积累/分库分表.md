---
title: 关于分库分表
date: 2022-03-01 11:06:58
type: "个人生活"
categories:
- 基础知识
  tags:
  thumbnail: https://tva1.sinaimg.cn/large/008i3skNgy1gt14d2z4q0j31900u0tb5.jpg
---

### 1、垂直分库

垂直分库相对来说是比较好理解的，核心理念就四个字：`专库专用`。

同表不同字段放在不同的库表

按业务类型对表进行分类，像**订单**、**支付**、**优惠券**、**积分**等相应的表放在对应的数据库中。

开发者不可以跨库直连别的业务数据库，想要其他业务数据，对应业务方可以提供 `API` 接口，这就是微服务的初始形态。

垂直分库很大程度上取决于业务的划分，但有时候业务间的划分并不是那么清晰，比如：订单数据的拆分要**考虑到与其他业务间的关联关系**，

并不是说直接把订单相关的表放在一个库里这么简单。也要考虑获得信息的成本

在一定程度上，垂直分库似乎提升了一些数据库性能，可实际上并没有解决由于**单表数据量过大导致的性能问题**（即列字段获取需要扫描的行数过多），所以就需要配合水平切分方式来解决。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0OzaL5uW2aNib4PtcBicWAMHxiaiaPtOCLKee5kbUUuoxaL124DGLYiaHrxk091loSnQGD059NHtxPODdaucLdWalvA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 2、垂直分表

`垂直分表`是基于数据表的列（字段）为依据切分的，是一种大表拆小表的模式。

通常根据ID来取模路由

我们知道数据库是以行为单位将数据加载到内存中，这样拆分以后核心表大多是访问频率较高的字段，而且字段长度也都较短，因而可以加载更多数据到内存中，

来增加查询的命中率，减少磁盘IO，以此来提升数据库性能。

**垂直切分的优点**：

- 业务间数据解耦，不同业务的数据进行独立的维护、监控、扩展。
- 在高并发场景下，一定程度上缓解了数据库的压力。

**垂直切分的缺点**：

- 提升了开发的复杂度，由于业务的隔离性，很多表无法直接访问，必须通过接口方式聚合数据。获取字段成本可能会增加
- 分布式事务管理难度增加。
- 数据库还是存在单表数据量过大的问题，并未根本上解决，需要配合水平切分。**行扫描过多**

### 水平切分

前边说了垂直切分还是会存在单库、表数据量过大的问题，当我们的应用已经无法在细粒度的垂直切分时， 依旧存在单库读写、存储性能瓶颈，这时就要配合水平切分一起了，水平切分能大幅提升数据库性能。

### 1、水平分库

水平分库是把同一个表按一定规则拆分到不同的数据库中，每个库可以位于不同的服务器上，以此实现水平扩展，是一种常见的提升数据库性能的方式。

这种方案往往能解决单库存储量及性能瓶颈问题，但由于同一个表被分配在不同的数据库中，数据的访问需要额外的路由工作，因此系统的复杂度也被提升了。

**可以解决单库存储量以及性能瓶颈的问题**

### 2、水平分表

水平分表是在**同一个数据库内**，把一张大数据量的表按一定规则，切分成多个结构完全相同表，而每个表只存原表的一部分数据。

例如：一张 `order` 订单表有 900万数据，经过水平拆分出来三个表，`order_1`、`order_2`、`order_3`，每张表存有数据 300万，以此类推。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0OzaL5uW2aNib4PtcBicWAMHxiaiaPtOCLKe4U1kpSBibKQjozDcNoicWdWbHUBEQ8kwVFtTiak8qiaU6eggH2PsNeoXDA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**水平分表**尽管拆分了表，但子表都还是在**同一个数据库实例中**。只是解决了单一表数据量过大的问题，并没有将拆分后的表分散到不同的机器上

还在竞争同一个物理机的**CPU**、**内存**、**网络IO**等。

要想进一步提升性能，就需要将拆分后的表分散到不同的数据库中，达到分布式的效果。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0OzaL5uW2aNib4PtcBicWAMHxiaiaPtOCLKeERff5tia2tYdzl8qW9ej9KOsymNnSmoic5h0QbM102IcST2b46PnEzhA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**水平切分的优点：**

- 解决高并发时单库数据量过大的问题，提升系统稳定性和负载能力。
  即通过路由的方式分担单库压力，减少行扫描的工作量
- 业务系统改造的工作量不是很大。

**水平切分的缺点：**

- 跨分片的事务一致性难以保证。
- 跨库的join关联查询性能较差。
- 扩容的难度和维护量较大，（拆分成几千张子表想想都恐怖）。拆了的表都要记录在一个地方维护，

## 一定规则是什么

我们上边提到过很多次 `一定规则` ，这个规则其实是一种路由算法，就是决定一条数据具体应该存在哪个数据库的哪张表里。

常见的有 `取模算法` 和 `范围限定算法`

还拿 `order` 订单表举例，先对数据库从 0 到 N-1进行编号，对 `order` 订单表中 `work_no` 订单编号字段进行取模，得到余数 `i`，`i=0`存第一个库，`i=1`存第二个库，`i=2`存第三个库....以此类推。

**优点：**

- 数据分片相对比较均匀，不易出现请求都打到一个库上的情况。

**缺点：**

- 这种算法存在一些问题，当某一台机器宕机，本应该落在该数据库的请求就无法得到正确的处理，这时宕掉的实例会被踢出集群，此时算法变成hash(userId) mod N-1，用户信息可能就不再在同一个库中了。
- 不好扩展，多一台机器取模算法要变化，原有的数据也要变动

### 2、范围限定算法

按照 `时间区间` 或 `ID区间` 来切分，比如：我们切分的是用户表，可以定义每个库的 `User`表里只存10000条数据，第一个库只存 `userId` 从1 ~ 9999的数据，第二个库存 `userId`为10000 ~ 20000，第三个库存 `userId` 为 20001~ 30000......以此类推，按时间范围也是同理。

**优点：**

- 单表数据量是可控的
- 水平扩展简单只需增加节点即可，无需对其他分片的数据进行迁移
- 能快速定位要查询的数据在哪个库

**缺点：**

- 由于连续分片可能存在**数据热点**，比如按时间字段分片，可能某一段时间内订单骤增，可能会被频繁的读写，而有些分片存储的历史数据，则很少被查询。

## 分库分表的难点

### 1、分布式事务

由于表分布在不同库中，不可避免会带来跨库事务问题。

一般可使用 "`三阶段提交` "和 "`两阶段提交`" 处理，但是这种方式性能较差，代码开发量也比较大。

通常做法是做到最终一致性的方案，如果不苛求系统的实时一致性，只要在允许的时间段内达到最终一致性即可，采用**事务补偿**的方式。

这里我应用阿里的分布式事务框架`Seata` 来做分布式事务的管理，后边会结合实际案例。

# SpringCloud Alibaba [Seata](https://so.csdn.net/so/search?q=Seata&spm=1001.2101.3001.7020)处理分布式事务

## 1.[分布式](https://so.csdn.net/so/search?q=分布式&spm=1001.2101.3001.7020)事务问题由来

我们之前都是一个应用程序连一个库，而且应用程序和数据库被放在同一台机器上，

但是在分布式微服务的系统架构中，将原来的三个模块拆分成三个独立的应用，分别使用三个独立的数据源。如下图所示

![image-20220504204026330](source/学习积累/分库分表.assets/image-20220504204026330.png)

1.仓储服务：对给定商品扣除仓储数量。

2.订单服务：根据采购需求创建订单。

3.账户服务：从用户账户中扣除余额。

上述操作都需要操作相应的数据库，但是如何保证数据库的全局一致性是一个很大的问题。

**一次业务操作**需要**跨多个数据源或需要跨多个系统进行远程调用**，就会产生**分布式事务问题**。

## 2.什么是Seata？

Seata是一款开源的分布式事务解决方案，致力于在微服务架构下提供高性能和简单易用的分布式事务服务。

## 3.分布式事务的相关概念

分布式事务处理过程的一ID+三组件模型，

一ID即Transaction ID XID，全局唯一的事务ID。

三组件：

1.TC (Transaction Coordinator) - 事务协调者

维护全局和分支事务的状态，驱动全局事务提交或回滚。

2.TM (Transaction Manager) - 事务管理器

定义全局事务的范围：开始全局事务、提交或回滚全局事务。执行

3.RM (Resource Manager) - 资源管理器

管理分支事务处理的资源，

与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。

![image-20220504213114250](source/学习积累/分库分表.assets/image-20220504213114250.png)

如图所示一个TC管理全局，一个XID事务ID带着TC，TM，RM。

1. TM向TC申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID。
2. XID在微服务调用链路的上下文中传播。
3. RM向TC注册分支事务，将其纳入XID对应的全局事务的管辖。
4. TM向TC发起针对XID的全局提交或回滚决议。
5. TC调度XID下管辖的全部分支事务完成提交或者回滚请求。



================

雪花算法自动生成ID优点

- 信息安全：如果`ID`是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定`url`即可；如果是订单号就更危险了，竞争对手可以直接指导我们一天的单量。所以在一些应用场景下，需要`ID`无规则不规则，让竞争对手不好猜。
- 含时间戳：这样就能够在开发中快速了解这个分布式`id`的生成时间
- 单调递增：保证下一个`ID`一定大于上一个`ID`,例如事务版本号，`IM`增量消息，排序等特殊需求。

## ID号生成系统的可用性要求

硬件要求

- 高可用：发一个获取分布式`ID`的请求，服务器就要保证`99.9999%`的情况下给我创建一个唯一分布式`ID`
- 低延迟：发一个获取分布式`ID`的请求，服务器就要快，极快，毫秒级别。不然双十一根本扛不住
- 高`QPS`：假如并发一口气`10`万个创建分布式`ID`请求同时杀过来，服务器要顶的住且一下子成功创建`10`万个分布式`ID`

## 一般通过方案

### UUID

```Java
 String s = UUID.randomUUID().toString();
```

如果只考虑唯一性，`uuid`是`ok`的，但是它是无序的，入数据库性能差。

### 数据库自增主键

数据库的自增`ID`机制的主要原理是，数据库自增`ID`和`mysql`数据库的`replace info`实现的。

`replace info`根`insert`功能类似，不同在于：`replace info`首先尝试插入数据列表中，如果发现表中已经有此行数据(根据主键或唯一索引判断)，则先删除，再插入；否则，直接插入新数据。

```sql
REPLACE INTO goods(STATUS,name,num,version) VALUES(1,"abc",1,1);
select LAST_INSERT_ID();
```

那数据库自增`ID`机制适合做分布式`ID`吗？

不合适。

 系统水平扩展比较困难，比如定义好了步长和机器台数之后，**如果要添加机器该怎么做？**假设现在只有一台机器发号是1,2，3,4,5(步长是1)，这个时候需要扩容机器一台，可以这样做，把第二台机器的初始值，设置得比第一台超过很多，貌似还好，现如果100台机器呢？没法玩了

- 数据库压力还是很大，**每次获取ID都得读写一次数据库，非常影响性能**，不符合分布式ID里面的延迟低和要高QPS的规则。

### 基于redis生成全局id策略

因为`redis`是单线程的天生保证原子性，可以使用原子操作`incr`和`incrby`来实现。

集群分布式下：

在`Redis`集群情况下，同样和`mysql`一样需要设置不同的增长步长，同时`key`一定要设置有效期。

可以使用集群来获取更高的吞吐量。

假如一个集群中有5台`redis`，可以初始化每台redis的值分别是`1，2，3，4,5`，然后步长都是5。

各个`redis`生成`ID`为：

A:1,6,11,16,21
B:2,7,12,17,22
C:3,8,13,18,23
D:4,9,14,19,24
E:5,10,15,20,25

> 集群有几台机器，步长就设置为多少,扩展机器不太方便

该方案是可行的

唯一缺点：为了一个`id`，配置麻烦，维护麻烦，就想要个`id`,却要维护一个`redis`集群。

如果有固定集群，则比较合适

## 雪花算法

`Twitter`的分布式自增`ID`算法，经过测试`snowflake`每秒能够产生`26`万个自增可排序的`ID`。

 Twitter的雪花算法生成ID能够按照时间有序生成

1. 雪花算法生成`id`的结果是一个`64bit`大小的整数，为一个`long`型
2. 分布式系统内不会产生`ID`碰撞并且效率较高

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200720164922272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjQxMjYwMQ==,size_16,color_FFFFFF,t_70)

- 第一个`bit`位（`1bit`）：`Java`中`long`的最高位是符号位代表正负，正数是0，负数是1，一般生成ID都为正数，所以默认为0。
- 时间戳部分（`41bit`）：毫秒级的时间，不建议存当前时间戳，**而是用（`当前时间戳 - 固定开始时间戳`）**的差值，可以使产生的`ID`从更小的值开始；`41`位的时间戳可以使用`69`年，`(1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69`年
- 注意时间戳的应用，毫秒级，不一定要取全部，可以取一个当前值 - 起始值，可以使位数减少。一定位数来表示，一定时间内都不会重复。
- 工作机器id（`10bit`）：也被叫做`workId`，这个可以灵活配置，机房或者机器号组合都可以。可以部署在2^10=1024个节点，包括5位`datacenterId`和5位`workerId`
- 序列号部分（`12bit`，2^12个/毫秒）：**自增值支持同一毫秒内同一个节点可以生成`4096`**个`ID`。12位可以表示的最大正整数2^12-1=4095，即可以用0,1，2，3…4094这4095个数字，来表示同一机器同一时间(1毫秒)内产生4095个ID序号。

雪花算法可以保证：

所有生成的id按时间趋势递增；

整个分布式系统内不会产生重复id(因为有datacenterId和workerId来做区分)

> github：https://github.com/twitter-archive/snowflake

```java
 if (lastTimestamp == timestamp) {
            // 当前毫秒内，则+1
            sequence = (sequence + 1) & sequenceMask;
            if (sequence == 0) {
                // 当前毫秒内计数满了，则等待下一秒
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0L;
        }
```



