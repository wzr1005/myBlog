

2. 请你介绍一下项目，以及实习负责的内容
   金融理财搜索App主要是用户搜索，还有后台配置化的开发，比如创建索引、热搜配置。输入一些关键词就可以识别意图，主要是通过一些词表，还有相关度计算再排序负责后台的后台管理系统，例如索引配置、搜索框和热搜推荐、前台的搜索功能

   1. 什么是es索引配置化：

   2. 后台可以配置**每个字段的搜索策略**，比如布尔查询中的 must必须条件、should匹配的越多，分数越高，以及query的是否分词，term、text。分词器、minimum_should_match匹配多少。
   
   3. 是否返回得分、页码大小，召回多少条文档
   
   4. 金融有哪些策略，这个涉及到算法的知识，它会根据query分词中的结果，去提前用模型生成的词表中
   
   5. boost条件权重控制，可以控制搜索条件的权重，将某个条件的权重加大，most field会除以总query数。
   
   6. `multi_match` 多匹配查询的类型有多种，其中的三种恰巧与 [了解我们的数据](https://www.elastic.co/guide/cn/elasticsearch/guide/current/_single_query_string.html#know-your-data) 中介绍的三个场景对应。 `best_fields` 、 `most_fields` 和 `cross_fields` （最佳字段、多数字段、跨字段(将多个字段放在一起查，命中尽可能多的）
      可以使用 `^` 字符语法为单个字段提升权重，在字段名称的末尾添加 `^boost` ， 其中 `boost` 是一个浮点数：
   
      ```json
      {
        "multi_match": {
          "query": "` + keyword + `",
          "fields": [
            "goodsName.goodsName^3",
            "categoryName^2",
            "goodsName.goodsNameMax^1",
            "jingle^1",
            "specString^1"
          ],
          "type": "cross_fields",
          "minimum_should_match": "5<90%"
        }
      }
      ```
   
      ![image-20220406222747686](学习积累ssets/image-20220406222747686.png)
   
   实习在一个搜索部门，组里是做知识图谱应用，本人负责的主要是应用数据构建，主要是工作上维护链路、线上问题追查和需求的开发，以及旧链路到新链路的迁移。
   
2. es分为索引区（倒排索引）和元数据区（json文档）

   1. es如何保证数据的一致性，即多个数据副本是否能保持一致的特性
      1. 在一致性的条件下，系统在执行数据更新操作之后，能够从一个一致性状态转移到另一个一致性状态
      2. 分布式系统不可能满足CAP，consistent、Available、分区容忍性partition Tolerable，最多能满足两项
      3. es怎么选举：
         1. master节点失效之后，每个结点会ping配置文件中的发现host的IP，找到存活的结点并过滤。ZenDiscovery是ES自己实现的一套用于节点发现和选主等功能的模块，没有依赖Zookeeper等工具。
         2. 如果有多个可用的master，可能会出现脑裂的状况。一般选择版本最新，其次ID最小的节点作为新的主节点。
      4. 什么时候发起选举：
         总结一句话，即当一个节点发现包括自己在内的多数派的master-eligible节点认为集群没有master时，就可以发起master选举。
      5. 选举过程：
         1. 候选节点参与主节点选举的消息，发给其他节点确认，超过一半就选举成功。

4. redis了解持久化、原理、集群

   1. redis的sortedSet，zadd "table" “member1" 11111.22222,实现多维排序，或a*100000+b * 10000 + c * 100 +d 实现多维排序，主要不要让score溢出

   2. redis过期时间

      1. 如果用DEL, SET, GETSET会将key对应存储的值替换成新的，命令也会清除掉超时时间；如果 list 结构中添加一个数据或者改变hset数据的一个字段是不会清除超时时间的；如果想要通过set去覆盖值那就必须重新设置expire。
      2. TTL cache_page    # 查看剩余生存时间
      3. EXPIRE cache_page 30000   # 更新过期时间

   3. redis集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。

   4. zset，不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。可以实现多维度排序

   5. Redis 提供了两种持久化方式:RDB（默认）适用于经常修改型 和AOF 1、aof文件比rdb更新频率高，优先使用aof还原数据。

      2、aof比rdb更安全也更大

      3、rdb性能比aof好

      4、如果两个都配了优先加载AOF

   6. **redis通讯协议(RESP )，能解释下什么是RESP？**
      RESP 是redis客户端和服务端之前使用的一种通讯协议；RESP可以序列化不同的数据类型，如整数（integers），字符串（strings），数组（arrays）。它还使用了一个特殊的类型来表示错误（errors）
      根据不同的前缀后缀传输不同的数据类型
      优势：
      实现容易、解析快

   7. redis主从

      1. 根据一个 Redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。 只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据
      2. 主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。
      3. redis哨兵解决高可用，
         1. 监控。Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。
         2. 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。
         3. 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。
         4. redis事务：单线程也需要保证其原子性。
            很多情况下我们需要一次执行不止一个命令，而且需要其同时成功或者失败。redis对事务的支持也是源自于这部分需求，即支持一次性按顺序执行多个命令的能力，并保证其原子性。
         5. 在事务的基础上，LUA脚本，支持更复杂的操作
         6. 
      4. 集群
         1. 可以突破单台服务器的上限，读写分离，把读和写的压力分开，
         2. 都是通过代理来实现，Twemproxy就是个路由，到某台机器，同时使用了keeplive保证了高可用。

   8. redis为什么是单线程的，因为单次操作已经很快了，性能的瓶颈主要是机器内存和网络带宽。

   9. redis为什么要持久化，

      1. 从内存写到磁盘，重启的时候，在redis重新启动的时候加载这些数据，从而最大限度的降低缓存丢失带来的影响。
      2. 备份，保证高可用

   10. redis的淘汰策略

      1. 从已过期的数据 lru、所有key进行lru
      2. 过期key随机淘汰、所有key淘汰
      3. 优先淘汰最近要过期的key
      4. 所有key使用LFU、过期key使用LFU

   11. redis常用的性能问题和解决方案。

       1. Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件
       2. 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次
       3. 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内
       4. 尽量避免在压力很大的主库上增加从库？
          redis 支持主从的模式。原则：Master 会将数据同步到 slave，而 slave 不会将数 据同步到 master。Slave 启动时会连接 master 来同步数据。
          每多一个slave，都会多一份复制的数据传输压力
       5. 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master <- Slave1 <- Slave2 <- Slave3…
       6. Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。
       7. 集群是异步复制
       8. 尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key,而是应该把这个用户的所有信息存储到一张散列表里面.
       9. 缓存穿透，查询不存在的key
          1. 不存在的key也写入缓存
          2. 布隆过滤器
       10. 缓存雪崩，大面积失效
           1. 缓存失效后进行加锁或者消息队列来避免
           2. 二级缓存
           3. 不同的key，设计一个随机时间，让缓存失效的影响面没那么大
       11. redis限流机制是怎么实现的
           1. 服务端限流
              接口请求进行限流，线程池就是一个天然的限流器，限制了并发个数，多了的会放在队列里面派对，队列放不下了就会拒绝。
              1. 实现
                 使用线程安全的Queue，固定大小1000，满了就拒绝，每秒钟清空一次queue
              2. 一个循环数组，每个下标代表一个秒的时间戳，请求来了就自增1，超过1000了就拒绝。或者redis每秒创建一个key，并对该时间的key进行请求计数，大于1000就拿不到token了。对于创建的历史key，超时自动删除。
              3. 缺点，不能平滑限流，第一秒400个，第二秒50个，那么只会处理150个（限流100/s)
              4. 服务端限流缺点：缺点也比较明显，由于服务提供者整体设置了最大限流数，此时所有的客户端共享同一份限流数据，那么有可能导致**有的服务能分配到资源有些服务请求分配不到资源**导致无法请求的情况。服务端包含很多服务，**导致服务分配不均**
           2. 客户端限量
              1. 客户端限流解决上服务端限流提到的问题，**它能保证每个客户端都能得到响应。**但是从其它方面考虑，必须针对不同的客户端做不同的限流策略：
              2. 请求量大，但时效性不高，此时将限流数控制小一些会比较合适请求量大，但时效性高，此时将限流数适当调高响应时间长，即慢接口，适当降低主流业务，核心业务，适当调高非主流业务，
           3. **RPC实现**
              1. 注解限流
       12. 读写分离架构的缺陷在于，不管是 Master 还是 Slave，每个节点都必须保存完整 的数据，如果在数据量很大的情况下，集群的扩展能力还是受限于单个节点的存 储 能 力 ， 而 且 对 于 Write-intensive 类 型 的 应 用 ， 读写分离架构并不适合。

5. mysql索引、crud操作数据的本质、sql调优、分库分表（用户 商品 订单例子）、日志、事务、乐观锁悲观锁

   1. 事务，开始begin，结束，commit。单条sql也是一个事务。。
      四个隔离级别：脏读，读未提交，就像腾讯文档，一起修改。不可重复读，读已提交，一个事务可能会改多次，另一个事务多次读取得到的值前后不一致。幻读，可重复读，对于已有的值可以保证一致性，但是新写的值则没有办法控制，比如ID 1-10，写入了一个，select出来发现数据不一致。这个问题可以用mvcc解决。串行化，最大程度保证一致性，但是这个不支持并发。

   2. MVCC：

      1. 当前读和快照读：**当前读**，基于悲观锁，
         就是当前读取的数据一定是最新版本的，加了读锁 select for update,共享锁，其他事务只可以读，不可以写。排他锁，其他事务不能读和写。**悲观锁**包括共享锁和悲观锁。JDK是synchronized，
         **乐观锁**适合读多写少的场景，比如CAS，版本链控制，CAS的ABA问题，因为CAS基于值的比较来判断数据是否正确，但是如果这个值从1改成2，2再改成1，那也可以认为正确，不排除会引发其他问题。
         MVCC就是基于乐观锁，使用版本链来控制数据的正确一致性。Java的就是原子类，
      2. 快照读，基于快照的版本来实现版本控制。

   3. 二阶段提交，分布式数据库，prepare阶段，参与者本地写redolog和unlog日志，把commit请求发给协调者，commit阶段，协调者根据投票情况把ack发给参与者。
      

   2PC并不是一个无阻塞协议，有些失败修复方式有可能能使得站点阻塞

      一个站点失败会导致全局阻塞
      三阶段提交，是一个无阻塞协议，多了一个pre commit阶段，引入了超时机制，在某个参与者超时之后会发个信息给他，他可以再次发送请求。
      3PC协议在站点失败，甚至是所有的站点都失败的情况下也不会带来阻塞，但是会因为通信失败导致全局事务原子性被破坏，不同的站点收到了不同的策略。
      向所有下属发送precommit消息，而不是commit消息。然后在收到足够数目(比必须处理的最大故障数目要大)的ack消息以后，协调者向日志中强迫写入commit记录，再向所有下属发送commit消息。
      在3PC中，协调者场地能够有效地推迟commit决定，只有在确定所有下属都知道commit决定以后，才真正发出commit决定。

      若任一参与者回答中止Abort消息，则进入第三段(执行段)，协调者发出反转Rollback命令。

      也就是说，在一个分布式系统中，如果有一半的主机发送了precommit ack之后，协调者可以认为有必要提交，让准备好的主机写入commit日志，并且等其他暂未收到的主机ready，推迟commit提交

   4. 分库分表怎么查询，所有库和表都查询吗

   5. Innodb和Mysiam

      1. Innodb支持事务，MySiam不支持事务。
      2. 锁粒度，Innodb行锁，Mysiam支持表锁。锁粒度太细加锁释放锁的代价太高。比如一个线程访问一个资源，可能是访问一整块资源，而这块资源如果分的太细，被其他线程持有，则很容易造成死锁问题，每个细小的资源使用都需要加锁释放锁，代价很高，比如给一整块资源加锁，执行完再给下一个线程。
      3. Innodb是聚簇索引，以id主键作为逻辑地址连续存储在磁盘中，Mysiam不是聚簇索引。聚簇索引就是非主键索引的叶子是主键，再通过主键索引，得到的叶子结点才是数据。非聚簇索引直接叶子结点就是数据。
      4. 都支持b+索引。索引存在磁盘中，使用的时候调入内存。
      5. Innodb获取行数select count（因为事务，行更新太频繁，代价太高），Myisam是有这类元数据，保存了行数之类的元信息
      6. Innodb必须要有主键，聚簇索引。myisam则不需要。
      7. 为什么myisam查询的效率比mysql高
         1. 因为innodb是聚簇索引，以b+树为例，根据字段定位到的是主键id，需要根据id去定位存储地址，只有ID连续的，内存中才是连续的，但是myisamb+树存储的是具体的值，相邻的值在磁盘中夜相邻。而且没有事务的束缚，

   6. SQL优化：

      1. 表中建索引，where和group by经常用到的字段，覆盖索引

      2. 尽量避免使用select *，where中有索引中不存在的字段的时候，索引就会失效

      3. 使用短索引

         对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引

      4. 索引失效的原因：

         1. Like模糊查询不要用左通配符，因为B+树索引的机制
         2. 大于 小于或者不等于的情况
         3. 对索引进行加减运算也会失效，运算或者函数处理过的数据都会失效
         4. 复合索引要(a,b,c) 只where了c，没有前导a、b

      5. Explain两个很重要的参数：

         1. type，有All、range、index、即扫描方式，system（内存中） > const > eq_ref > ref > range > index > ALL
         2. Extra，NULL、Using index、Using where、Using index condition（命中索引、Using filesort（文件排序、Using temporary（临时内存

      6. **什么时候不要使用索引？**

         1. 经常增删改的列不要建立索引
         2. 有大量重复的列不建立索引；（不好找）
         3. 表记录太少不要建立索引。直接读到内存中就可以。

      7. 1. 索引需要占用***\*磁盘空间\****，因此在创建索引时要考虑到磁盘空间是否足够
         2. 创建索引时需要**对表加锁**，因此实际操作中需要在业务空闲期间进行

         
         
         

5. Linux，至少准备15个指令，常用的不常用但实用的
   ll 、 grep、 ps -ef、tail、ls、diff -r 、man、find、whoami、sudo、which、echo、source

   1. ls

      1. 以易读的方式显示文件大小(显示为 MB,GB…)：`ls -lh` 
      2. 以最后修改时间升序列出文件：`ls -ltr` 。
      3. 在文件名后面显示文件类型：`ls -F`

   2. df

      1. `df -k` 将以字节为单位输出磁盘的使用量。
      2. `df -h` 选项可以以更符合阅读习惯的方式显示磁盘使用量
      3. `df -T` 选项显示文件系统类型

   3. 可以看到，rsync在大量小文件的情况下，速度要比scp快很多，大约只需要7%的时间。还可以选择排除文件

   4. grep

      1. 在一个文件夹中递归查询包含指定字符串的文件：`grep -r "ramesh" *` 
      2. 在文件中查找字符串(不区分大小写)：`grep -i "the" demo_file` 

   5. sed 给文件中的文本进行替换、删除操作

   6. wget -O taglist.zip http://www.vim.org/scripts/download_script.php?src_id=7701

   7. **Linux系统里，您知道 buffer 和 cache 如何区分吗？**

      Buffer 和 Cache 都是内存中的一块区域。Buffer是用来写的，cache是用来读的，都是在内存中的区域。

      - 当 CPU 需要写数据到磁盘时，由于磁盘速度比较慢，所以 CPU 先把数据存进 Buffer ，然后 CPU 去执行其他任务，Buffer中的数据会定期写入磁。
      - 当 CPU 需要从磁盘读入数据时，由于磁盘速度比较慢，可以把即将用到的数据提前存入 Cache ，CPU 直接从 Cache中 拿数据要快的多。

   8. sar命令在这里用于查看 TCP 连接状态，其中包括：

      - active/s：每秒本地发起的TCP连接数，既通过connect调用创建的TCP连接；
      - passive/s：每秒远程发起的TCP连接数，即通过accept调用创建的TCP连接；
      - retrans/s：每秒TCP重传数量；

   9. top 命令包含了前面好几个命令的检查的内容。比如系统负载情况（uptime）、系统内存使用情况（free）、系统 CPU 使用情况（vmstat）等。因此通过这个命令，可以相对全面的查看系统负载的来源。同时，top 命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最多的进程、CPU占用率最高的进程等。

   10. Linux内核。用户空间到内核空间的转换。1. 系统调用。 2. 硬件中断。

   11. Linux 使用的进程间通信方式？管道、FIFO消息队列、信号量、共享内存、套接字

   12. inode是个索引块，逻辑地址和物理地址的转换

   13. 硬链接是指针，软链接可以看作一个句柄，文件的路径

   14. **针对网站访问慢，怎么去排查？**

       1. 首先要确定是用户端还是服务端的问题，对比不同机器访问速度
       2. 如果是服务端问题：
          看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢
          服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况
       3. 如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。
       4. 可以关闭一些守护进程

7. elastic search文档型数据库的索引存储形式、数据类型keyword、text，分词，以及日志场景

   1. 其实 es 第一是准实时的，数据写入 1 秒后可以搜索到。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。
      1. 数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）
      2. 每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。
      3. 数据写入 segment file 之后，同时就建立好了倒排[索引](https://so.csdn.net/so/search?q=索引&spm=1001.2101.3001.7020)
   2. 多个分片怎么查询，默认会查询能组成索引的所有节点，prefer参数可以设置只查主节点或者主节点优先，或者指定id的节点进行查询。client.prepareSearch(“index”).setPreference(“_primary”)
      1. 当客户端发起创建document的时候，ElasticSearch需要确定这个document放在该索引哪个shard上。这个过
         程就是数据路由。
         路由算法：分片位置shard=hash(routing) % number_of_primary_shards
         如果number_of_primary_shards在查询的时候取余发生变化，则无法获取到该数据。即：在查询的时候，底层
         根据文档id%主分片数量获取分片位置
   3. ES计算相关度，
      1. 布尔模型(Bool Model)，判断文档里面是否存在这三个term之一或者更多，只有存在关键词的文档才可以进入下一轮的竞争排序。这个bool模型很大程度保证了计算的实时性和有效性
      2. {“name”:”charles”,”description”:”hunterplus web”}
         后面的三个计算都以这两个简单文档为例子
         1. 出现了关键词的文档数量除以总残留文档数量
         2. 简称IDF就是这个词在文档中的重要程度，是对某个词条在全数据库中所有文档出现的频率作的计算。 IDF ＝ 1+log ( total/ (fre+1) )
         3. IDF表征的是区分度、稀缺性，用以评估一个单词在语料库中的重要程度，一个词在少数几篇文档中出现的次数越多，它的IDF值越高，如果这个词在大多数文档中都出现了，这个值就不大了。可参考上述**fre和total变量。**
         4. TF刻画了词语w对某篇文档的重要性，IDF刻画了w对整个文档集的重要性。TF与IDF没有必然联系，TF低并不一定伴随着IDF高。实际上我们可以看出来，IDF其实是给TF加了一个权重。
      3. 三个因子都是文档在index的时候就马上计算出来的，会占用一定的存储空间
      4. 为什么要引出BM25
         1. 在一个相当长的文档中，像 `the` 和 `and` 这样词出现的数量会高得离谱，以致它们的权重被人为放大。
         2. 这就是所谓的词频饱和度，TF-IDF的词频饱和度是线性的，而BM25的词频饱和度是非线性的：**我们想要一个算法，不受停用词影响，**

7. 数据结构，冒泡、快排、

8. 操作系统，进程线程、异步、内存管理、IO中断

   1. 线程如何排队。双向链表，知道前后的状态。等待模式有共享锁、排他锁
      ![img](https://pic3.zhimg.com/80/v2-7c7541a220d6631398d307434936fcca_1440w.jpg?source=3af55fa1)
   2. 等待中的线程如何感知到锁空闲并获得锁
      唤醒操作：1. 尝试释放当前线程持有的锁，2. 如果成功释放，那么去唤醒头结点的后记节点，找到队列中符合条件的第一个线程唤醒。
      实现可重入：
   3. 进程上下文切换怎么优化：
      1. 进程切换时才需要切换上下文，换句话说，只有在进程调度的时候，才需要切换上下文
      2. 可以改变调度方式，比如自旋、cas等尽量避免切换上下文
   4. 进程和线程，必问！
      1. 进程是一个程序在一个数据集合上运行的过程，它是**系统进行资源分配和调度的一个独立单位**。线程是为了程序更好的支持并发执行，提供资源利用率和系统吞吐量，减少程序在并发时的时空开销。
         1. 线程是轻量级进程，是一个基本的cpu执行单元，可以说具体的任务都是线程来执行的，进程负责提供资源和调度。
         2. 线程由线程ID、pc程序计数器、寄存器和堆栈组成，一个进程内线程共享所有资源，当然线程也可以拥有自己的资源，比如放在堆栈中的局部变量、Thread Local也可以认为是线程独有的。
            进程由代码段、分页数据段组成、还有pcb，top命令能看到对应的pid，一个进程对应一个端口方便通信。
         3. 线程的地址空间是基于进程的，线程的切换一般不会引起进程的切换，
      2. 怎么创建一个进程，进程一般是一个可执行文件，比如jar包、python包，exe，相应的机器指令可以执行。
      3. 进程的通信，Linux通常用管道，固定大小的缓存区写数据，
   5. 线程的通信
      1. 共享内存，比如全局变量
      2. 消息传递，就是wait和notify信号
      3. volatile保证了线程之间的可见

10. 计算机网络，三次握手四次挥手、七层五层模型、TCP层的拥塞控制

   1. 快速重传机制
      1. 累计确认机制，接收端收到比期望序号大的报文段时，
      2. 为什么是三次冗余ACK，
   2. TCP头部20-60字节，序号占32位，端口号16位，这种递增方式的 ISN很容易让攻击者猜测到 TCP 连接的 ISN，因此大多数实现是会在基准值的基础上添加随机数。
   3. TCP与UDP的区别
      
      1. UDP在传送数据之前不需要先建立连接，对方的传输层收到UDP报文后，不需要给出任何确认。
   4. TCP粘包、拆包发生原因
      1. 拆包，
         1. 要发送的数据大于TCP发送缓冲区剩余空间大小，将会发生拆包。
         2. 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。65535
      2. 粘包，就是多条信息连成一块成一条信息。可用专用分隔符
         1. 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包。
         2. 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。
      3. 解决方法。
         1. 发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。
         2. 发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。
         3. 

10. 计算机系统，存储器、流水线

12. 消息队列，说一说Kafka

    1. 怎么保证消息不会丢失

       1. 三个阶段都有可能存在消息丢失的情况

          1. 生产阶段：消息队列使用确认机制，来保证消息可靠传递，调用的时候，消息队列的客户端会把消息发送给Broker，Broker接收到消息会给客户端确认
             只要Producer收到了Broker的确认响应，就可以保证消息在生产阶段不会丢失，没收到确认会自动重试，重试失败会抛出异常让客户端处理，保证了生产阶段消息不丢失。
             
          2. 存储阶段。如果对消息可靠性要求非常高，可以通过配置Broker参数来避免因为宕机而丢失消息，
             1. 对于单节点的Broker，需要配置Broker参数，保险起见，先写再确认。收到消息后，将消息写入磁盘再给Producer返回确认响应，
             2. 如果是集群，可以配置成，**至少两个以上节点**收到消息，再给客户端发送确认响应
       
          3. 消费阶段。和生产阶段类似，即收到消息之后，需在执行消费逻辑后在发送确认消息。

          4. 如何发送顺序消息
             这个其实也很简单。
             kafka可以通过partitionKey，将某类消息写入同一个partition，一个partition只能对应一个消费线程，以保证数据有序。也就是说生产者在写消息的时候，可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。
          
          5. Kafka如何保证单partition有序？
             producer发消息到队列时，通过加锁保证有序。
          
             那么是否有这样一个问题呢？
             先后两条消息发送时，前一条消息发送失败，后一条消息发送成功，然后失败的消息重试后发送成功，造成乱序。
          
             为了解决重试机制引起的消息乱序为实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Number。
          
             消费者端创建多个内存队列，具有相同 key 的数据都路由到同一个内存 队列；然后每个线程分别消费一个内存队列即可，这样就能保证顺序性。
          
          
          
          ​	

17. Java，JVM、GC、锁升级、HashMap、集合类家族，并发编程、线程池，泛型编程

    1. 关键字

       1. static 修饰的变量是jvm一启动就加载的变量，是Class的变量，不管实例化多少个对象，始终就只有一个类变量。而没有static修饰的变量对象，是对象Object的变量，每实例化一个对象就会产生一个xxx的变量。
          访问的权限也不一样。静态方法只能访问静态变量和实例化之后的方法。
       2. 通常final对象都用static修饰，以节省内存。即只会加载一次。
          1. final特性1，不可被修改
          2. 不可被继承
          3. String设计为不可变类型是为了线程安全。
             1. 本质上是想让String具备像int char那样的基本类型。频繁使用的类 保证线程安全
             2. 而字符串也许是大段重复使用的，作为常量，可以节省内存，提高效率。因此不可变也是其条件之一。

    2. 集合类，

       1. int可以隐式向上转为long，而long不能直接转为int，因为有数据溢出的风险。强制类型转换或Long.intValue()

       2. ```java
          int[] test_int = new int[] { 1, 2, 3, 4, 5};
          
          test_int = Arrays.copyOfRange(test_int, 1, 4);
                  
          System.out.println(Arrays.toString(test_int));
          //输出 [2,3,4]
          ```

       3. 队列和栈。Deque双向队列，add Last和add是正常队列顺序，addFirst是压栈的形式放入数组.pollFirst或peek First是栈顶，pollLast是队列的顺序。

       4. 类

          1. 静态内部类可以有静态成员，非静态父类的内部类不可以有静态成员

       5. Map的遍历方式

          1. for(Map.Entry<k,v> entry: map.entrySet()){}
          2. for(K key: map.keySet())
          3. Iterator<Map.Entry<k,v>> iterator = map.entrySet().iterator();
          4. map.forEach((k, v) -> System.out.println("key: " + k + " value:" + v)); lambda表达式。

       6. 优先级队列和比较器，Comparable，Comparator

          1. Queue<Node> pq = new Priority<>(new Comparor<Node>{

             ​		@Override

             ​		public int compare(Node a, Node b){

             ​				if(a.v b.v1)

             ​				else a v2 b v2

             ​		}

             })

             或者
             Queue<Node> pq = new Priority<>(){}
             Node需要重写compareTo方法。

             Arrays.sort(num, new Comparator) **只适用于in t[]数组等基本类型，**

          2. 对于非基本类，可以用Collections.sort(nodes, new Comparator<Node>() {
             List<?T>都可以

          3. o1 - o2 升序，大于0才需要交换。

          4. int[]，double[]，char[]等基数据类型的数组，Arrays类之只是提供了默认的升序排列，没有提供相应的降序排列方法。

          5. 要对基础类型的数组进行降序排序，需要将这些数组转化为对应的封装类数组，如Integer[]，Double[]，Character[]等，对这些类数组进行排序。

          6. 基本类型可以使用Collections.reverse()排序

       7. 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 `16 * 0.75 = 12` 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。

          因此通常建议能提前预估 HashMap 的大小最好，尽量的减少扩容带来的性能损耗。

       8. 从这两个核心方法（get/put）可以看出 1.8 中对大链表做了优化，修改为红黑树之后查询效率直接提高到了 `O(logn)`。
          但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。

       9. EntrySet 进行遍历。

          可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低。

       10. JDK 推出了专项专用的 ConcurrentHashMap ，该类位于 `java.util.concurrent` 包下，专门用于解决并发问题。ConcurrentHashMap 同样也分为 1.7 、1.8 版，两者在实现上略有不同。

          1. 如图所示，是由分段锁 Segment 数组（一锁多个Entry）、HashEntry 组成，和 HashMap 一样，仍然是数组加链表。

          2. 1.8抛弃了原有的 Segment 分段锁，而采用了 `CAS + synchronized` 来保证并发安全性。也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。

             其中的 `val next` 都用了 volatile 修饰，保证了可见性。

       11. 哪些集合类是线程安全的

           1. stack，比ArrayList多了同步机制
           2. hashtable
           3. 枚举

       12. 1. ReentrantLock和synchronized的区别。**锁的细粒度和灵活度**：很明显ReenTrantLock优于Synchronized在
           2. Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。
           3. Synchronized经过编译后，会在同步块前后分别形成monitorenter和monitorexit两个字节码指令，在执行monitorenter指令时，首先要尝试获取对象锁，如果对象没有别锁定，或者当前已经拥有这个对象锁，把锁的计数器加1，相应的在执行monitorexit指令时，会将计数器减1，当计数器为0时，锁就被释放了。如果获取锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。
           4. ReentrantLock**等待可中断**，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。
              Synchronized不可中断
           5. 多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。
           6. 总结：reentrantLock实现更加灵活、粒度更小，以前效率比synchronized高，CAS实现，尽量在用户态解决冲突的问题。可以实现公平锁，即原生的队列实现AQS，而非公平锁可能会有老线程饥饿的情况。AQS是一个线程等待队列，还有一个状态同步器依赖volatile关键字，AbstractQueuedSynchronizer。
           7. 前面的锁都是基于排他锁，效率不太高。因此有了一种读写分离的锁，ReadWriteLock。就类似数据库中的共享锁和排他锁。
           8. 为什么锁对象要用final关键字
           9. synchronized是对什么加锁，修改的是什么对象的对象头
       10. 为什么AQS是双向队列
               在AQS中，还有其它一些功能。比如，线程A此时占有锁，线程B也来lock，因为A占有，B会判断是否需要阻塞，如下代码为判断是否需要阻塞
               A have Lock，B come，B判断A是否释放，决定自己是否阻塞。有前驱节点比较方便。
           11. 原子类atom，一般情况下原子类AtomicInteger常见的操作是自增，不会有ABA问题。
               1. 原子类可以有无限循环的问题，尤其是在高并发的情况下，怎么解决。1.8Long Adder来解决。
                  1. longAdder将原子类的变量拆分为多个，再进行合并。降低该变量的并发度，减少了CAS的失败次数。
                  2. **每个Cell里面有一个初始值为0的long类型变量****这个改变增加了当前线程重试CAS成功的可能性。最后在获取LongAdder当前值时，是把所有Cell变量的value值累加后再加上base返回的。**
           
       13. 多线程

           1. Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。三个功能，判断任务是否完成，能够中断任务，能够获取任务执行结果。

           2. 创建一个线程池对象，控制要创建几个线程对象。

                public static ExecutorService newFixedThreadPool(int nThreads)

              为什么引入executor线程池框架，

              1. 每次new Thread()耗费性能，
              2. 调用new Thread创建的线程缺乏管理，称为野线程，可以无限制创建，之间互相竞争，会过多占用系统资源，
              3. 不利于扩展，比如定时执行，定期执行

              采用线程池的优点

              1. 重用存在的线程，减少对象创建、销毁的消耗
              2. 可有效控制最大并发线程数，提高系统资源的使用率
              3. 提供定时执行，单线程，并发数控制等功能

           3. Java通过Executors提供四种线程池

                1. newCachedThreadPool创建一个可缓存线程池，**简而言之，就是来多少个任务，就创建多少个线程，超过60秒就回收**

                   1. 它是一个可以无限扩大的线程池；

                   2. 它比较**适合处理执行时间比较小的任务；** 

                   3. ```java
                      public static ExecutorService newCachedThreadPool(){
                          return new ThreadPoolExecutor(0,Integer.MAX_VALUE,60L,TimeUnit.MILLISECONDS,new SynchronousQueue<Runnable>());
                      }
                      ```

                   4. keepAliveTime为60S，意味着线程空闲时间超过60S就会被杀死；

                   5. 采用SynchronousQueue装等待的任务，这个阻塞队列没有存储空间，**这意味着只要有请求到来，就必须要找到一条工作线程处理他**，如果当前没有空闲的线程，那么就会再创建一条新的线程。

                2. newFixedThreadPool 创建一个定长线程池，**可控制线程最大并发数**，超出的线程会在队列中等待。适合10000个任务，由固定10个线程去做，速度可能比较慢。（mysql最大连接数是10w)

                   1. [阻塞队列](https://www.zhihu.com/search?q=阻塞队列&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A245992718})采用了LinkedBlockingQueue，它是一个无界队列；

                   2. 由于阻塞队列是一个无界队列，因此永远不可能拒绝任务；

                   3. ```java
                      public static ExecutorService newFixedThreadPool(int nThreads){
                          return new ThreadPoolExecutor(nThreads,nThreads,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>());
                      }
                      ```

                   4. 由于采用了无界队列，实际线程数量将永远维持在nThreads，因此maximumPoolSize和keepAliveTime将无效。

                3. newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。

                   ```java
                   public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {
                           return new ScheduledThreadPoolExecutor(corePoolSize);
                       }
                   ScheduledThreadPoolExecutor类的构造：
                       public ScheduledThreadPoolExecutor(int corePoolSize) {
                           super(corePoolSize, Integer.MAX_VALUE, 0, TimeUnit.NANOSECONDS,
                                 new DelayedWorkQueue());
                       }
                   ```

                   1. 采用的延时队列，
                      1. DelayedWorkQueue优先队列是定制的优先级队列，只能用来存储RunnableScheduledFutures任务，堆是实现优先级队列的最佳选择，而该队列正好是基于堆数据结构的实现。
                      2. 延迟队列是基于数组实现的，初始容量为16；获取延迟队列中头部节点的线程称为leader，说明leader线程是不断变化的
                      3. 但leader线程在等待，则其他线程也会等待，直到leader线程获取根节点，且从等待线程中产生新的leader线程。获取延迟队列中头部节点的线程称为leader,且不断变化，
                   2. Timer的内部只有一个线程，如果有多个任务的话就会顺序执行，这样我们的延迟时间和循环时间就会出现问题。
                   3. ScheduledExecutorService是[线程池](https://so.csdn.net/so/search?q=线程池&spm=1001.2101.3001.7020)，所以就不会出现这个情况，在对延迟任务和循环任务要求严格的时候，就需要考虑使用ScheduledExecutorService了。

                4. newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。

           4. newCachedThreadPool创建一个可缓存的线程池，调用executor重用以前构造的线程。如果现有的线程没有可用的，则创建一个新线程并添加到池子中，终止从缓存中60秒未被使用的线程。

           5. 多线程之阻塞队列，支持两个附加操作的队列

              1. 当队列为空时，获取元素的线程会等待队列变为非空
       2. 当队列满时，存储元素的线程会等待队列可用
              3. 常用于生产者与消费者场景，生产者是往队列中添加元素的线程，消费者是从队列中拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只能从容器里拿元素
       4. ｜BlockingQueue｜插入方法｜add抛出异常｜offer返回特殊值｜put一直阻塞｜offer(e, time)超时退出｜
                 |BlockingQueue|移除方法｜remove抛出异常｜poll返回特殊值｜take一直阻塞｜poll(e, time)超时退出｜
          |BlockingQueue|检查方法｜element抛出异常｜peek返回特殊值｜
              5. Delayque 一个实现priority Blocking Queue实现延迟获取的无界队列，在创建元素时，可以指定多久才能获取队列当前元素，只有在延时期满后才能从队列中获取元素。（Delay Queue可以运用在以下应用场景，
                 1. 缓存系统的设计，可以用Delay Queue保存缓存元素的有效期，使用一个线程循环查询delayqueue的有效期。时间做key？一旦从队列中获取到数据，表面有缓存到期了，定时任务调度，使用Delay Queue保存当天将会执行的任务和执行时间，一旦从DelayQueue中获取到任务就开始执行，比如TimeQueue就是使用Delay Queue实现的。
                 2. **ArrayBlockingQueue**，是用一个数组实现的有界阻塞队列，此队列按照先进先出FIFO的原则对元素进行排序，支持公平锁和非公平锁。公平锁就是按照队列顺序来的，不公平锁会根据权重来调整顺序。存储元素的数组。可以看到是 final 修饰的，意味着一旦 ArrayBlockingQueue 创建之后，大小不能再改变了
                    1. 与LinkedBlockingQueue相同，使用Condition的方法来同步和通信：await()和signal()
                    2. ArrayBlockingQueue生产者和消费者使用的是同一把锁；LinkedBlockingQueue生产者和消费者使用的是不同的锁
                    3. 数组实现和链表实现，Link大批量的时间复杂度比较高。Array初始化需要长度。
                    4. LinkedBlockingQueue中使用了一个AtomicInteger对象来统计元素的个数
                 3. SynchronizedQueue，一个不存储元素的阻塞队列，每一个put操作必须等待take操作，否则不能添加元素。支持公平锁和非公平锁，SynchronizedQueue一个使用场景就是在线程池里，Executor.newCachedThreadPool()就使用了SynchronizedQueue，这个线程池需要根据（新任务到来——创建新线程，如果有空闲线程则会重复使用，线程空闲了60秒后就会被回收。
              6. condition也是一种信号，配合lock，可以实现synchronized一样的功能。
              7. Blocking Queue，实际上是一种生产者消费者模式，当队列程度大于指定值时，生产者阻塞，反之队列长度为0时，消费者阻塞，内部使用Reentrant Lock+Condition实现，
              8. Count Down Latch调用对数值减一，当数值减为0时，就会唤醒所有因为调用await（）方法阻塞的线程。达到一组线程等待另一组线程的效果。
              9. Cyclic Barrier。所有线程会等待全部线程到达栅栏之后才会继续执行，并且最后到达的线程会完成 Runnable 的任务。线程可以重复利用![img](https://img-blog.csdnimg.cn/img_convert/0dde2d343b11b15140fcfec3ef247eba.png)
              10. 阻塞队列
              
           1. LinkedBlockingQueue 用的比较多
                     1. LinkedBlockingQueue是一个无界缓存等待队列。当前执行的线程数量达到corePoolsize的数量时，剩下的元素会在阻塞队列里等待。所以使用此阻塞队列时，maximumPoolSize就相当于无效了。每个线程完全独立于其他线程。生产者和消费者使用独立的锁来控制数据的同步。即在高并发的情况下可以并行操作 队列中的数据。
                   这个队列需要注意的是，虽然通常称其为无界队列，但是可以人为指定队列大小，而且由于其用于记录队列大小的参数是int类型字段，所以通常意义上的无界其实就是队列长度为Integer.MAX_VALUE,且在不指定队列大小的情况下也会默认队列大小为这个值。
                            2. SynchronousQueue
                   SynchronousQueue没有容量，是无缓冲等待队列，是一个不存储元素的阻塞队列，会直接将任务交给消费者，必须等队列中的添加元素被消费后才能继续添加新的元素，？？拥有公平（FIFO）和非公平（LIFO）策略，使用SynchronousQueue阻塞队列一般要求maximumPoolSize为无界（IntegerMax），避免线程拒绝执行操作。
                            3. ArrayBlockingQueue
                   ArrayBlockingQueue是一个有界缓存等待队列，可以指定缓存队列的大小，当正在执行的线程数量等于corePoolSize时，多余的元素缓存在ArrayBlockingQueue队列中等待有空闲的线程时继续执行，当ArrayBlockingQueue已满时，加入ArrayBlockingQueue失败，会开启新的线程去执行，当线程数达到最大的maximumPoolSizes时，再有新的元素尝试加入ArrayBlockingQueue时会报错。
                            4. DelayedWorkQueue
                   DelayedWorkQueue当特点是内部元素并不是按照放入时间排序，**而是会按照延迟的时间长短对任务进行排序**，leader线程监听堆顶。内部采用的是“堆”的数据结构。之所以线程池ScheduledThreadPool和SingleThreadScheduledExecutor选择DelayedWorkQueue，是因为它们本身正是基于时间执行任务的，而延迟队列正好可以把任务按时间进行排序，方便任务的执行。
                  
              11. 线程池拒绝策略，线程池中，有三个重要的参数，决定影响了拒绝策略：corePoolSize - 核心线程数，也即最小的线程数。
                  
                  对于一个CPU，线程数总是大于或等于核心数的。
              
                  一个内核至少对应一个线程，但通过超线程技术，一个内核可以对应两个线程，即可以同时运行两个线程。
                  
                  对于英特尔CPU：除了核心数之外，还可以使用线程数的概念，因为它是通过英特尔超线程技术实现的。
                  
                  对于AMDCPU：只有内核数，没有线程数的概念。因为AMDCPU没有超线程技术，一个CPU核对应一个线程。
                  
                  
                  workQueue - 阻塞队列 。 maximumPoolSize - 最大线程数
                  当提交任务数大于 corePoolSize 的时候，会优先将任务放到 workQueue 阻塞队列中。当阻塞队列饱和后，会扩充线程池中线程数，直到达到 maximumPoolSize 最大线程数配置。此时，再多余的任务，则会触发线程池的拒绝策略了。总结起来，也就是一句话，当提交的任务数大于（workQueue.size() + maximumPoolSize ），就会触发线程池的拒绝策略。
                  拒绝时机：
                  
                  1. 调用shutdown关闭线程池时，继续submit就会拒绝 
                  2. 2. 任务队列已满，且达到最大线程池数量。
              3. AbortPolicy ，直接丢弃任务，抛出异常，有感知
                     当任务添加到[线程池](https://so.csdn.net/so/search?q=线程池&spm=1001.2101.3001.7020)中被拒绝时，它将抛出 RejectedExecutionException 异常。（该策略下，直接丢弃任务，并抛出RejectedExecutionException异常）
                     场景：**这是线程池默认的拒绝策略**，在任务不能再提交的时候，抛出异常，及时反馈程序运行状态。**如果是比较关键的业务，推荐使用此拒绝策略，这样子在系统不能承载更大的并发量的时候，能够及时的通过异常发现。**
           2. DiscardPolicy，丢弃任务，什么也不做，无感知
                     当任务添加到线程池中被拒绝时，默认情况下它将丢弃被拒绝的任务。（即该策略下，直接丢弃任务，什么都不做）
                     使用此策略，可能会使我们无法发现系统的异常状态。**建议是一些无关紧要的业务采用此策略**。例如，本人的博客**网站统计阅读量**就是采用的这种拒绝策略。
                  3. DiscardOldestPolicy，队列执行poll，空出一个位置，再将被拒绝的任务添加进队尾
              此拒绝策略，是一种喜新厌旧的拒绝策略。是否要采用此种拒绝策略，还得根据**实际业务是否允许丢弃老任务来认真衡量**。发布消息可以使用这种
                  4. CallerRunsPolicy，不进入线程池执行。自己另起一个线程执行。**不允许失败场景**（对性能要求不高、并发量较小）。
              12. 为什么要setDaemon设置为守护进程，因为守护进程优先级低，出现异常会直接死亡。主线程结束，守护线程也会结束，也不会影响主线程的服务。
       13. join作用是让其他线程变为等待, t1.join();// 让其他线程变为等待，直到当前t1线程执行完毕，才释放。
           
       16. Java IO模型

           1. BO 阻塞 一直阻塞
           2. NBO 周期性的询问有没有准备好数据，数据从内核拷贝到进程空间也要阻塞
           3. BO Mulit IO多路复用，
              1. selector、一个线程监听轮询每一个socket的状态变化，有变化可立即处理
              
              2. poll、返回一个socket数组，对数组中的每个值进行检测，效率更高。
              
              3. epoll、不需要每次查询都要切换内核态获取事件描述信息，一次调用之后，事件就会与描述符信息绑定起来，把等待的描述符注册回调信息，这样发生事件之后会把信息加入就绪事件链表中，最后写到进程空间。
                 epoll用红黑树+链表来管理所有的句柄，就是文件描述符，监听对象的文件描述符，当该文件描述符事件完成后会收到传递过来的数据，内核把该fd插入到就绪链表中，epoll_wait会接收到数据，然后把数据拷贝到进程空间，也就是用户空间。清空链表，
                 有个回调函数，数据就绪了就会调用它
              
              4. Redis使用的是[epoll](https://so.csdn.net/so/search?q=epoll&spm=1001.2101.3001.7020)网络模型：
                 ![img](https://img-blog.csdnimg.cn/20210406004424699.png)
              
              5. (1) Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件
              
                 (2) 如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一 次
              
                 (3) 为了主从复制的速度和连接的稳定性，Master 和 Slave 最好在同一个局域网 内
              
                 (4) 尽量避免在压力很大的主库上增加从库
              
              6. ### MySQL 里有 2000w 数据，redis 中只存 20w 的数据，如何保证 redis 中的数据都是热点数据淘汰策略
              
              7. 京东购物车下单，保证高并发用户体验好的方案
              
                 1. 下单后减库存；用户下单，然后库存加锁，判断库存是否充足，用户下单完成，减库存，最后释放库存锁。
                    1. 1）假如100个人同时下单，只有一个人能下单成功。
                    2. 2）此时订单应该有一个过期状态，如果订单过期，库存加锁并回写库存后释放锁。
                 2. 方案2：支付才减库存；用户支付，然后库存加锁，判断库存是否充足，用户支付完成，减库存，最后释放库存锁。
                    1. 1）100个人可以同时下单，但是100个人同时付款时，只有一个人付款成功。
                    2. 正常情况下，商品加入购物车的用户>>>下单的用户>=付款的用户。
                    3. 如果从库存加锁的角度来说，在下单的时候加锁，那么高并发下用户体验可能比较差，因为同时下单只有一个人能下单成功，而且服务器性能可能会比较差。
                       1. 因为下单的人最多，不断的加锁，阻塞，性能最差。而且下单不一定付款
                    4. 普通的电商项目我认为方案一就足够了，因为下单的流程简单，而支付可能涉及到很多业务，如果支付里面锁库存，考虑的东西会有点多。有可能会被恶意下单不付款，导致最后一件都卖不出去也有可能
                    5. 但是在高并发下或者秒杀场景下，那可能就要在支付的时候锁库存。从业务角度来说，肯定是手快有，手快无；从代码的角度来说，支付跟减库存高度耦合，出现超卖、库存不一致情况大大降低，如果是下单锁库存，万一用户取消订单，那是不是库存要加回去，这种情况下高并发出现库存与实际消费不一致的可能性比较大。而且还有一个好处，秒杀场景在支付时候加锁能够保证所有的产品都卖出去，而在下单的时候加锁，那么有可能有用户取消订单，到秒杀结束时有产品没卖出去，如果是老板肯定是要全部卖出去的。

       17. AQS。AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch
           ReentrantLock的子类Sync类，继承了AQS抽象类，定义了一些线程的细节和方法，实现了park,unpark（挂起、执行线程）,Lock、unLock、CAS等方法，还提供了各种接口

           1. 获取锁。初始状态下state=0,所以线程A可以顺利获取锁，A获取锁后将state置为1。在A没有释放锁期间，线程B也来获取锁，此时因为state=1，表示锁被占用，所以将B的线程信息和等待状态等信息构成出一个Node节点对象，放入同步队列。同时阻塞线程B(这里的阻塞使用的是**LockSupport.park**()方法)。后续如果再有线程要获取锁，都会加入队列尾部并阻塞。

           2. 释放锁。即将state置为0，此时A会唤醒头节点的后继节点（所谓唤醒，其实是调用**LockSupport.unpark**(B)方法），即B线程从LockSupport.park()方法返回，此时B发现state已经为0，所以B线程可以顺利获取锁，B获取锁后B的Node节点随之出队。

           3. Sync类的Lock（）实现，CAS实现

              ```java
              final void lock() {
                          if (compareAndSetState(0, 1))
                              setExclusiveOwnerThread(Thread.currentThread());
                          else
                              acquire(1);
                   }
              ```

              可以看到compareAndSetState底层其实是调用的unsafe的CAS系列方法。比较前后状态。
              
              sync是关键字，关键字的作用是修饰变量用的，表示他具有某种特性。编译的时候

       18. 序列化

           1. 特殊的数据成员，如余户的密码、银行卡号，不想用序列化机制来保存它。为了在一个特定对象的一个成员变量上关闭序列号，可以在这个成员变量前加上关键字transient。
              静态变量天然不可序列化
              有些对象不能串行化，例如Thread对象，FileInputStream对象以及FileOutputStream对象等，因为其状态是暂时的。
           2. 序列化的作用，Java对象变成字节流（json、xml）的形式传出去，或者恢复一个Java对象。在Java的output Stream类下面的子类ObjectOutput Stream类就有对应的write Object，要求对象实现Java的序列化的接口
           3. 实现serializable、或Externlizable，可以定义哪些属性可以序列化，哪些不可以序列化，不可以序列化的不处理，

       19. 为什么 Lambda 表达式(匿名类) 不能访问非 final  的局部变量呢？

           1. 因为实例变量存在堆中，而局部变量在栈中分配，**Lambda表达式，会在另一个线程中执行，**如果在线程中要直接访问一个局部变量，可能线程执行时该局部变量已经被销毁了，换句话说，如果在匿名类或 Lambda 表达式中访问的局部变量，如果不是 final 类型的话，**编译器自动加上 final 修饰符**。

       20. ThreadLocal 维护一个map，<key,value> key为当前线程，value为此线程仅该线程能访问的变量。使用InheritableThreadLocal可以实现多个线程访问ThreadLocal的值。
           每个线程都有一个 ThreadLocal 就是每个线程都拥有了自己独立的一个变量，竞态条件被彻底消除了，在并发模式下是绝对安全的变量。
           Thread{

           ​	ThreadLocalMap <ThreadLocal弱引用, object(独占的变量，放在堆中)>
           }

           ThreadLocalMap类似于hashmap，但是没有链接结构，没有那么冲突解决办法，考虑到Local的变量值

           ![img](https://pics4.baidu.com/feed/3801213fb80e7beca4260b1d84121e3e9a506b15.jpeg?token=78511916a7ac4012a2aaf7e7c1f81649)

           1. 正常线程的变量是在栈中，独有。但是ThreadLocal是利用了线程的名称作为key来索引变量，达到独占的目的，其实是放在堆中的。
           2. 子线程继承父ThreadLocal线程可以实现多个线程访问ThreadLoal变量。
           3. 会不会内存泄露，如果直接使用ThreadLocal实例作为对象，是会造成内存泄露的，但是事实上使用的是ThreadLocal等弱引用，弱引用gc时候被回收了key为null，他持有的value就无法回收，造成内存泄露。需要注意的是，这里**立即释放了对threadLocal实例的强引用，帮助gc回收**。为什么不使用强引用，因为引用的ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。就是Value Object没有了，但是key还有，除非手动释放。
           4. 由于Thread中包含变量ThreadLocalMap，因此ThreadLocalMap与Thread的生命周期是一样长，如果都没有手动删除对应key，都会导致内存泄漏。
           5. 正确用法，每次使用完`ThreadLocal`都调用它的remove()方法清除数据。
           6. 适用场景：
              1. 线程间数据隔离，各线程的 ThreadLocal 互不影响
              2. 方便同一个线程使用某一对象，避免不必要的参数传递
              3. 全链路追踪中的 traceId 或者流程引擎中上下文的传递一般采用 ThreadLocal
              4. Spring 事务管理器采用了 ThreadLocal
              5. Spring MVC 的 RequestContextHolder 的实现使用了 ThreadLocal

       21. 单例为什么要使用双重校验？

           1. 因为如果在单例已经创建的情况下，使用synchronized会很耗时，第一重相当于粗过滤，
           2. 而第二次校验，设想这种情况，A线程和B线程同时经过第一次校验，竞争锁，A竞争锁成功，创建了实例。执行完释放synchronized，此时线程B拿到synchronized锁，这时因为volatile关键字已经可以判断instance是否非空，避免二次创建。
           3. 内部类的写法，借助于JVM类不重复加载的特性，来保证线程安全

       22. jdk8的新特性

    3. JVM

       1. 新建对象的五种方式：
          1. new关键字 
          2. 使用class类的newInstance方法 无参构造
          3. 使用Constructor类的newInstance方法
          4. 使用clone方法 **未使用构造方法**，需要实现cloneable接口
          5. 使用反序列化 **未使用构造方法**
       2. 类加载
          1. 针对每一个类的加载，都是加锁的。类加载器就是把类文件加载到虚拟机中
             ![img](https://img2018.cnblogs.com/blog/1415794/201907/1415794-20190723223959126-1589115961.png)
          2. **符合引用都是在常量池，虚拟机遇到new指令，会检测这个指令的参数是否在常量池存在，检测这个类是否已经被加载**、解析初始化过，如果没有就执行类加载过程。
          3. 类加载器，启动类加载器、扩展类加载器、应用程序加载器，以及用户的自定义类加载器。
          4. 一个java文件要先编译成.class文件，加载进内存，
             ![img](https://pic4.zhimg.com/80/v2-ecf6c3d0f5146029e9693d6223d23afb_1440w.jpg)
          5. 验证：
             1. 为了保证加载进来的字节码符合虚拟机规范，不会造成安全错误
                1. 文件格式验证，比如常量中是否有不被支持的常量值，boolean有没有除了true和false之外的值。
                2. 元数据的验证，比如该类是否继承了final类型的类，类中的字段方法是否与父类冲突。是否出现了不合适的重载
                3. 字节码的验证，保证语义的正确性，比如类型对应，类型转换
                4. 符号引用的验证，比如符号引用是否能够找到对应的类，private、public访问权限是否正确
          6. 准备，分配内存，并且根据不同的数据类型赋予不同的初始值。
          7. 解析，符号引用替换为直接引用。相对地址转为绝对地址。
             还有类名、方法名、字段名转换成为具体的内存地址。
          8. 初始化，zhge 阶段主要是对类变量的初始化，执行类构造器。如果是自类，要先执行副类的构造方法。
       3. 内存模型，运行时数据区。
          1. 1.7**方法区**属于JVM，1.8方法区在元空间，就是直接内存，一般IO的buffer也在直接内存里面。属于OS的空间而不是虚拟机。
             1. 方法区存储的是类的信息，字节码，运行时常量池基本包装类的常量、string、静态变量、方法数据、方法代码。1.8在元空间中，直接内存，操作系统的概念。读写数据也在内存中。
             2. 栈 存储的局部变量，上下文、方法出口，debug的时候
                本地方法栈，一些本地方法，hashcode，cas等
             3. pc寄存器
             4. 堆，对象的实例，最大的区域，和栈一样是动态区域
          2. 内存屏障
             1. 什么是内存屏障，它其实就是个cpu指令，机器指令，确保指令执行的顺序，影响数据之间的可见性。Load Barrier 读屏障。Store Barrier 写屏障
             2. 单线程的情况下，处理变量的指令load、store是不会有什么问题的，不会影响他们之间的依赖关系，但是多线程情况下，会导致出问题，参考volatile的使用场景
             3. 写屏障会强制把屏障前的数据刷新到缓存，确保线程读到的数据为最新值，而不需要考虑是哪个cpu执行的，保证一致性。
             4. **内存屏障正是通过阻止屏障两边的指令重排序来避免编译器和硬件的不正确优化而提出的一种解决办法**。重排序能提高运行速度，内存屏障能保证一致性。
             5. Java的内存屏障主要有load和store两类，读屏障可以使缓存的数据失效，强制去内存中读取最新的 数据。
             6. 使用场景：
                1. synchronized关键字包住的区域，读屏障，确保读取最新值，
                2. volatile关键字，对于定义的 变量，机器指令load、store阻止重排序，就是一种插入写屏障，写入数据之后强制刷新缓存，使其他线程读取的数据为最新，保证了数据的可见性。
       4. GC垃圾回收器
          1. 垃圾回收算法
             1. 标记清除法，标记即清除，清除速度快，但是空间不连续，容易产生内存碎片
             2. 标记整理法，区别在于整理，解决了内存碎片的问题。涉及到对象引用的改变，速度比较慢
             3. 复制算法。两块区域，存活对象移动到第二块区域，然后清除之后再交换回来，优点不会产生碎片，缺点 占用双倍空间。
             4. JVM是分代回收，因为新生代新垃圾比较频繁，为了内存碎片与性能考虑，使用复制算法。老年代因为存活的对象多，垃圾比较少，所以使用标记清除即可。新生代存活次数超过16晋升老年代。
          2. 四种引用
             1. 强引用，不会被垃圾回收器回收，会报Out Of Memory错误。new的实例就是强引用
             2. 软引用，GC内存满时会回收，soft Reference，一般缓存可以用软引用。比如引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。**就是把刚刚读取的对象缓存起来**，内存不够了再回收它。
             3. 弱引用，比较大的文件，需要读一次删一次，比如图片、视频之类的 文件。weak reference，只要GC就会被回收，ThreadLocal对象作为key，threadLocal因此造成内存泄露。
             4. 虚引用，随时都有可能被回收。虚引用主要用来跟踪对象被垃圾回收器回收的活动。没有具体的实际作用。
          3. 垃圾回收器
             1. 串行，单线程，适合单人电脑。--XX -serial 开启串行
             2. 吞吐量优先，多线程，堆内存较大，多核cpu支持。**单位时间内**STW卡顿时间最短，吞吐量最大。
             3. 响应时间优先，尽量单次STW卡顿时间最短，最快响应。
          4. JVM的永久代会发生垃圾回收吗
             1. 类卸载的时候会。
       5. JVM调优
          1. 一般调整堆栈的大小比例，
          2. 调整老年代和新生代的比例，

    4. 反射

       1. 反射是java的一个特性，通过反射可以动态的加载未知的外部配置对象，**临时生成字节码**进行加载使用，让代码更加灵活，不需要提前设定对象的类型
          反射则是一开始并不知道我要初始化的类对象是什么，自然也无法使用 new 关键字来创建对象了
          我们通过 Class 对象的 getFields() 方法可以获取 Class 类的属性，但无法获取私有属性。
          比如类的名称放在XML文件中，属性和属性值放在XML文件中，需要在运行时读取XML文件，动态获取类的信息。
       2. 获取过程：
          1. 首先调用Java.lang.Class的静态方法，获取类信息，Class.forName(“引用”)。触发类的初始化
             XXX.class 从而获取当前的类加载器。不会触发类的初始化但XXX类已经被加载到方法区。
             通过Object类的getClass方法，例如Object.getClass()。触发类的初始化
          2. JVM再调用类加载器，如果已经加载过了就不要再加载了
       3. 可以获取的信息
          1. class对象，
             1. Class c1 =bianliang.class, 
             2. Class c1 = xx.getClass()
             3. Class c1 = Class.forName("java.xx.xx")
          2. Field 成员变量，
             1. Field[] fs = bianliang.getFields() 公共成员，包括父类的字段
             2. ge tDeclaredField() 获取某个类的所有声明的字段 包括public private protect但不包含父类
          3. Method 包含静态方法和成员方法

       

13. Java网络收发请求的过程

    1. ObjectOutPutStream获得网络套接字流，进行对象的重构。writeObject将对象写入流中，对象的默认序列化机制写入的内容是：对象的类，类签名，以及非瞬态（如线程的上下文，没必要）和非静态字段的值。其他对象的引用也会导致写入那些对象。可使用共享机制对单个对象的多个引用进行编码，这样即可将对象的恢复为写入它们时的状态。
    2. 序列化操作不写出没有实现 java.io.Serializable 接口的任何对象的字段。**不可序列化的 Object 的子类可以是可序列化的。**
    3. 在 **writeObject 和 readObject 方法的实现中**抛出 **NotSerializableException**，可以阻止对象的序列化。ObjectOutputStream 将捕获异常并中止序列化进程。
    4. 每个类可以实现readObject、writeObject方法实现自己的序列化策略，即使是transient修饰的成员变量也可以手动调用ObjectOutputStream的writeInt等方法将这个成员变量序列化。

15. Spring

    1. 为什么要使用AOP

       - 可以实现业务代码和日志记录等代码分离，只需要写一个动态代理类，不需要修改源代码，给程序动态统一添加增加前置和后置功能，DynamicProxyInvocationDemo demo = new DynamicProxyInvocationDemo(p);
         利用反射产生一个目标类的代理，丢进去就可以了。不用过多关注增强功能的代码，实现解耦合。
       - 如何理解代理。
         1. 静态代理。就是将接口或者某一类型的实现类的某一方法做了一些包装，加了一些前置或者后置的增强方法，比如加一些日志记录的方法，或者做一些拦截。
            1. 好处。增加代码的复用，可以不需要在每一个需要增强的方法前后都加那些相同的代码，
            2. 加少对功能代码的入侵
         2. JDK动态代理。代理类和目标类之间有一个中间类，传入几个必要的参数，完成代理的调用
            1. 得到目标的对象的类加载器
            2. 得到目标对象的实现接口，
            3. 第三个参数需要实现InvocationHandler接口的对象。
         3. Cglib动态代理
            通过“继承”可以继承父类所有的公开方法，然后可以重写这些方法，在重写时对这些方法增强，这就是cglib的思想。
            根据里氏代换原则（LSP），父类需要出现的地方，子类可以出现，所以cglib实现的代理也是可以被正常使用的。
            LSP原则:*只要父类能出现的地方,子类就可以出现,并且替换为子类也不会产生任何错误或异常。*
            因为如果我们通过反射 arg1.invoke(arg0, ...)这种方式是无法调用到父类的方法的，子类有方法重写，隐藏了父类的方法，父类的方法已经不可见，如果硬调arg1.invoke(arg0, ...)很明显会死循环。
         4. 总结：
            1. jdk代理只能对实现了接口的类进行代理，而cglib代理可以对普通类进行代理
            2. jdk代理是通过反射的方式来实现动态代理，而cglib则是通过为**目标类生成一个子类的方式**来实现动态代理；
            3. 由于cglib代理是为目标类生成了一个子类，并对父类方法进行增强，所以目标类不能用final修饰；

    2. 微服务

       1. @EnableDiscoveryClient 发现注解，注册到eureka服务中心，
       2. client.getServices()获取服务清单，client.getInstances（name），获取实例，包括主机端口uri，服务id

    3. Bean
       BeanDefinitionMap。BeanFactory IOC容器的基础接口，提供IOC容器的基本功能。
       IOC容器的核心实现类，提供多个map集合用来存储bean的定义对象，提供getBean的方法的核心实现。
       Bean的生命周期
       beanDefinition、beanFactory、IOC容器内的成熟的Bean
       1. spring启动，扫描bean，以bean的名称为key，bean的属性信息存到**Bean Definition**的map当中，
       2. 再去map中遍历，验证是否单例（Bean默认是单例）、是否原型（原型在启动的时候是不会走spring初始化流程的，用的时候才会）、是否懒加载、bean的名字是否合法
       3. **bean对象的实例化**，只是开辟空间，没有分配值
       4. **Bean工厂对Bean进行处理，执行指定的初始化方法，**通过反射去实例化一个对象，半成品，bean Factory三级缓存
       5. **如果spring支持循环依赖的话，会把半成品的bean存到一个Objectmap当中**，这个map我们一般称为二级缓存（存储代理对象），三级缓存存储工厂类。
       6. 推断构造方法，比如加了@AutoWired注解的就加入到候选candidates集合中;之后进行属性注入，就是执行一些set方法，如果发现依赖的y不存在，
       7. 接着做生命周期初始化的回调，比如@postConstruct
       8. 回调完之后，**还有一些对Bean增强的处理**也需要执行，比如AOP，存在二级缓存
       9. **准备就绪之后，执行自身的业务方法**
       10. 接下来放在单例池当中（Spring的bean默认都是单例的）
       11. 如果**Bean实现类销毁方法**，就执行spring的销毁方法

    4. @Component 作用于类，@Bean作用于方法。

    5. Bean的循环依赖问题

       1. 在上述第7步，推断了构造方法之后，发现y没在spring容器当中，就会执行y的生命周期，就是从beanDefinition中把y拿出来，也是推断y的构造方法，把y实例化，此时发现在单例池中没发现x，又会走x的生命周期，看看x有没有出现在Objectfactory，此时x已经在ObjectFactory（工厂也利于扩展）中了，
       2. 单例的，非构造方法注入的才行，如果是构造方法注入，缺少这个对象，是没有办法构造出一个bean的。

    6. 单对象的循环依赖Spring通过3级缓存来解决，比如一个类A中有一个属性是B类。B类中有一个属性是A类，
       首先，Spring注入一个类的大体步骤分为

       1. 先完成对类的构造工作
       2. 对类的属性进行设置和填充

       首先将A类曝光到Singletonfactories中，完成A的构造后，需填充B属性，继续第二步，发现B还没构造，于是开始B构造流程，发现需要填充A，**从第三层缓存singletonFactories中找到A，**
       此时A还没构造完成，但是可以拿到A的一个引用。B拿到A的引用之后，完成B自己的填充属性工作，完成初始化工作。
       B拿到A的引用之后，完成B自己的填充属性工作，完成初始化工作，把自己放在**第一层缓存songletonObjects中**。

       这时回到A的这边，在拿到B对象后，完成自己的填充工作。

       | 源码                   | 级别     | 描述                                                       |
       | ---------------------- | -------- | ---------------------------------------------------------- |
       | singletonObjects       | 一级缓存 | 用于存放完全初始化好的bean，从该缓存取出的bean可以直接使用 |
       | early SingletonObjects | 二级缓存 | 存放原始的bean对象（尚未填充属性）                         |
       | singletonFactory       | 三级缓存 | 存放bean工厂对象，用于解决循环依赖                         |

       为什么一定要三级缓存？


       以上建立在构造器不依赖属性。否则会报循环依赖异常
       或者如果对象都是多例对象也会报
    
       spring getbean的时候，会先从一级缓存去找，一级没有就二级，如果二级也没找到，意味着这个bean还没有实例化，于是，spring会把这个早期bean放入二级缓存，在@autowire注入完成之后存入一级缓存（成品），三级缓存，用来代理bean。二级缓存发现没有bean，会由创建工厂来创建（工厂是一种可扩展的设计模式），
    
       实例化A的时候，早期bean，会把Abean放入Bean工厂里，注入的时候，发现需要B，在一二级缓存没发现B的Bean，实例化Bean，尚未属性注入，放在Bean工厂。
       此时B需要注入A，一级二级缓存找，没找到，再到三级缓存bean工厂找到了A，从三级缓存获得A，放入二级缓存中，此时A还是半成品，并没有完成属性填充@Autowire注入，A注入到B中，
       B完成了属性填充，执行初始化方法，成为一个成熟bean，放入一级缓存中，删除二级缓存对象，。对象A得到B，也初始化注入属性，生成一个成熟Bean，放入一级缓存，至此就加载完成了。
    
       **为什么需要三级缓存** spring一开始并不知道A是不是循环依赖，三级缓存的目的是为了延迟代理对象的创建，二级缓存存的是代理之后的bean。
       其实也可以在实例化之后再扔进AOP代理创建对象，也是没有问题的，但是这不符合spring的代理思想，就是在初始化的过程中执行AOP的增强代码。正常思路应该是在初始化结束后就执行完了相关的增强代码还有类加载等回调方法。
    
    7. 设计一个简单的IOC容器
    
       1. 什么是IOC，就是个第三方容器，对象们需要什么实例就可以让IOC容器去创建，其他的对象需要就可以去取。就像使得对象们就没有了耦合关系，依赖关系降低。不需要相互通信。也可以将IOC称为和粘合剂，协调对象进行运转
       2. 需要的组件
          1. BeanFactory，就是获取bean，注册AbstractBeanDefinitionReader的工具通过（name，BeanDefination）
          2. BeanDefination，Bean的基本数据结构
          3. Bean资源加载器
          4. AbstractBeanFactory implements BeanFactory，用hashmap存储了Bean，doCreate方法通过反射创建了对象，对对象属性值进行属性注入，如果属性是引用类型，还需要调用getBean递归的去查询那个bean，这样就完成了一次获取实例化Bean操作，并且也实现了**类依赖注入**（引用类型递归寻找）
             ApplicationContext接口是由BeanFactory接口派生出来的，所以提供了BeanFactory的所有功能。

15. Java的泛型是如何工作的？什么是类型擦除，为什么要类型擦除

    1. 编译器在编译时擦除了所有类型相关的信息，所以在运行时不存在任何类型相关的信息/例如List<String> 在运行时仅用一个List来表示。这样做的 目的，是确保能和Java5之前的版本开发二进制类库进行兼容。你无法在运行时访问到类型参数，因为编译器已经将泛型类型转换成了原始类型。
    2. 什么是泛型中的限定通配符和非限定通配符
       1. 限定通配符
          1. <? extends T> 它确保类型必须是T的子类来设定类型的上界
          2. <? super T> 它通过确保类型必须是T的父类来设定类型的下界。
       2. 非限定通配符
          <?> 可以用任意类型来替代。
       3. 泛型类型： T、 K、 V、E
       4. List<Object>可以存储任何类型的对象包括String, Integer等等，而List<String>却只能用来存储Strings。
          Array不支持泛型
          原始类型和带参数类型<Object>之间的主要区别是，在编译时编译器不会对原始类型进行类型安全检查，却会对带参数的类型进行检 查，通过使用Object作为类型，可以告知编译器该方法可以接受任何类型的对象，比如String或Integer。
          List listOfRawTypes = new ArrayList();需要显式转换
       5. Java中List<?>和List<Object>之间的区别是什么?
          你可以把List<String>, List<Integer>赋值给List<?>，却不能把List<String>赋值给 List<Object>。   
       6. signature。任何类、接口、构造器方法或字段的声明如果包含了类型变量（type variable）或参数化类型，则Signature属性会为它记录泛型签名信息。
          签名有助于实现反射、调试以及编译，现在Java的反射API能够获取到泛型类型，最终的数据来源就是这个属性。

16. 动态代理 设计模式的一种

    1. jdk动态代理，只需要把我们的对象放到代理接口就可以了，共用一套增强代码，不需要修改源代码，代码入侵较小。

18. 项目

    1. 请求转发和重定向问题 转发是转发接口路径，**重定向是重定向网址**
       1. 重定向是两次请求，**转发是一次请求**，**转发速度要高于重定向**
       2. 重定向之后地址栏上的地址会发生变化，变化成第二次请求的地址。
          转发之后地址栏上的地址不会变化，还是第一次请求的地址
       3. 请求转发，无论XXX是否以"/"开头，都跳转到[http://ip:port/](https://link.zhihu.com/?target=http%3A//ip%3Aport/)项目名/XXX
          重定向，如果XXX以"/"开头，则表示目标地址为[http://ip:port/XXX](https://link.zhihu.com/?target=http%3A//ip%3Aport/XXX)；如果不以"/"开头，则表示目标地值为[http://ip:port/](https://link.zhihu.com/?target=http%3A//ip%3Aport/)项目名/XXX
       4. **请求转发只能在站内跳转**，重定向可以跳转到任意想要的地址。
    2. tomcat服务器启动加载项目
       1. startup启动命令，tomcat内部的main方法
       2. 加载webapps路径下的项目
          1. 加载项目描述文件web.xml
          2. 扫描项目中使用了Servlet相关注解的类
          3. 所有servlet都被tomcat管理起来，产生请求路径和Servlet的映射关系
             1. 客户端http请求，映射到某个Servlet路径
    3. springboot一次请求的过程：
       1. 前端把一些信息包装成请求头，其中包含请求方式、请求url http协议版本等
       2. springboot会根据路径加载相应的Controller进行拦截，定位到controller方法根据方法
       3. Controller一般会拿着请求体去调用service方法，执行必要的业务逻辑
       4. Service接口层通常负责在内存中处理数据和执行业务逻辑，由具体的实现类实现，访问磁盘或缓存会调用Resposity的接口，去数据库增删改查
       5. Service层执行完业务逻辑之后，把数据进行包装，再返回给controller，组装成返回体返回到页面
    4. https交换密钥的过程，重点是交换加密算法，占了两次握手。
       1. 浏览器发起 HTTPS 请求，给服务端自己的加密算法
       2. 服务端返回 HTTPS 证书（包含服务端的公钥）；服务端给公钥给客户
       3. 客户端验证证书是否合法，如果不合法则提示警告。客户端验证公钥之后，使用协商后的加密算法生成公钥，使用服务端的公钥加密
       4. 服务器再使用私钥解密，返回消息给客户端。
       5. [非对称加密](https://so.csdn.net/so/search?q=非对称加密&spm=1001.2101.3001.7020)的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互（每次传输都需要传递通信双方的公钥），非对称加密的效率是无法接受的。
       6. 为了提高通信效率，保证一定的安全性，HTTPS在交换密钥环节使用非对称加密方式，之后的建立通信交换报文阶段则使用对称加密方式。具体做法是：发送密文的一方使用对方的公钥进行加密处理“对称的密钥”(随机数)，然后对方用自己的私钥解密拿到“对称的密钥”（随机数），这样可以在确保交换的密钥安全的前提下，使用对称加密方式进行通信。所以，HTTPS 采用对称加密和非对称加密两者并用的混合加密机制。
    5. sql注入
       1. 将恶意代码放在前端表单里面，提交给后端，后端在执行代码时有可能触发这段代码。解决办法，
    6. 简而言之，标识网络中的一台计算机，比较常用的就是IP地址和MAC地址，
       1. 但计算机的IP地址可由用户自行更改，管理起来就相对困难，
       2. 而MAC地址不可更改，所以一般会把IP地址和MAC地址组合起来使用。
       3. ip地址每个位置有他代表的含义，方便路由。mac地址比较随机，出厂自带的。
    7. 2MSL 2倍报文存活时间，怕最后一个ack服务端没收到，超过这个时间会重发断开连接的报文。有两个理由
       1. 安全的终止这次全双工连接，FIN_WAIT CLOSE_WAIT
       2. 如果不等待，马上结束这次连接，马上又建立一次连接，会有旧的豹纹发过来，导致信息错乱
    8. 为什么是三次握手
       1. 第一次客户端给服务器一个起始编号，服务器接收到，服务器回给客户端的起始编号客户端不一定能收到。
       2. 如果服务器的起始编号不能确定的话，贸然建立起来的连接则不可靠，不知道服务器第一条发的消息是哪条。
       3. 也可以防止在会话结束之后，客户端的延时建立请求连接再次到达服务器端，服务端又再次建立连接，导致网络资源的浪费。
       4. SYN_RECV->ESTABLISHED 服务器收到客户请求，SYN_SEND->ESTABLISHED
    9. TCP 拥塞控制
       1. 慢开始 回到1 -》2 4 8 16 一直到一个门槛值 一般是拥塞窗口大小的一半
       2. 拥塞避免 到了门槛值之后，每次增加1，一直到逼近拥塞窗口的大小。即加法增大
       3. 快重传 发送方收到三个重复确认，说明服务端没收到。 网络不一定是真的拥塞，快速重发一个试试。 
       4. 快恢复 收到三个重复确认，说明只是丢失了个别豹纹，将窗口调整到门槛值，开始加法增大，即拥塞避免。
    10. XSS 跨站脚本攻击。
        1. 将邮件或其他信息包装成链接，当用户点击时，会暴露自己的cookies和token，浏览器会拿着这个去执行返回的恶意脚本代码。
    11. TCP沾包
        1. 为什么会沾包，批量发送。建立连接后发送不同的数据结构的消息，豹纹切分的时候出现的错误，
        2. 发送方应加入特殊标志位，如x0，或加入消息长度。或者固定长度，用0填充。
    12. 进程切换主要有哪些开销
        1. 进程将代码等其他信息加载进内存，放在页表里面，上下文PC，进程切换，页表也要切换，需要重新加载页表的缓存。
        2. 切换内核栈和硬件上下文，开销很大
        3. 线程只需切换内核栈和上下文，页表等进程数据不需要。
    13. 系统调用
        1. 如fork进程，执行了一次创建新进程的系统调用。
        2. 外围系统的中断处理，进入内核态。
        3. 异常，运行时不可预知
        4. Linux进程通信，
           1. 管道
           2. 共享内存
           3. 消息队列。
           4. 套接字，rpc调用。
           5. 信号量，本质是一种锁机制。
    14. 拦截器实现
        1. 通过反射获取bean，判断是否是需要拦截的类，比如一般需要拦截的是controller类，获取方法名、参数，
        2. cookie.setMaxAge(0);//删除cookie
        3. session可以放在redis里面，本地定义cookie。登出把redis缓存的session删除
        4. 读取状态信息逻辑：就是拿到cookie缓存的token，再去redis获取登陆信息，做其他操作也需要先进行这些操作
    15. Session和Cookie关系详解
        1. 什么是Session。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。
           1. 默认情况下SESSION保存在服务器的硬盘中，没有特别的存储长度限制，理论上可以存储任何数据，但并不建议任何数据都保存在SESSION中，原因不说了（考虑一下用户数及其庞大的情况下，每访问一个php文件，就要读取SEESION，特别是SEESION写入内存的情况下。），当然也可以写入memcache，甚至单独的SESSION服务器。
           2. SESSION通常用来保存与用户信息相关的： 就是一登陆就希望能看到的信息
              1. 身份信息、登陆状态 
              2. 用户的个性配置、权限列表
              3. 其他的一些通用数据（比如购物车）
           3. 我通常把通用的、频繁存取的、小数据量的跟用户相关的数据放入SEESION，视场景而定，我手头的一个项目，是把模块的信息（属性、菜单、结合权限生成栏目列表）写入SEESION的。
           4. 因为HTTP是无状态的协议，也就是说，这个协议是无法记录用户访问状态的，其每次请求都是独立的无关联的，一笔是一笔。而我们的网站都是设计成多个页面的，所在页面跳转过程中我们需要知道用户的状态，尤其是用户登录的状态，这样我们在页面跳转后我们才知道是否可以让用户有权限来操作一些功能或是查看一些数据。
           5. **所以，我们每个页面都需要对用户的身份进行认证**。当然，我们不可能让用户在每个页面上输入用户名和口令，这会让用户觉得我们的网站相当的SB。为了实现这一功能，用得最多的技术就是浏览器的cookie，我们会把用户登录的信息存放在客户端的cookie里，这样，我们每个页面都从这个cookie里获得用户是否登录的信息，从而达到记录状态，验证用户的目的。
           6. 实现网站的单例登录：
              1. 在用户表里增加一个判断登录状态的字段，如 loginStatus 0,表示未登录，1，表示已登录。
              2.  当用户登录时查询此属性，如果为 0 可登录，如果是 1 表示已有人登录，不可在登录。
              3. 用户如果已登录，将loginStatu为1的用户实例放入到 Session 中。
              4. 当用户点击后台退出按钮时，我们可以update用户实例的loginStatus 为 0。此为正常流程。
              5. 但如果用户没有点击后台退出按钮，直接关闭浏览器退出，该如何办。？？
                 1. 使用 **HttpSessionListener** 监听
                 2. 服务器 会在每次新的会话（客户端打开一个新的页面）时 产生一个带有唯一ID的默认Session，并触发 HttpSessionListener 的 public void sessionCreated(HttpSessionEvent event) 方法。
                 3. 当**默认Session过期时**触发HttpSessionListener的 public void sessionDestroyed(HttpSessionEvent event) 方法。
                 4. 如果服务器意外重启，应在服务器重启后自动更新用户的登录状态。！！！

18. 设计模式

    1. 什么是设计模式
       使用设计模式可以提高代码的复用、降低耦合、增加可读性，增加开发的效率的一些策略。
    2. 单例模式，一个项目中只需要一个，可以复用，降低新建回收的开销。不可变，不易拓展
    3. 工厂模型，只需要把要实现的接口或对象丢进工厂，就会执行的所需要的操作。IOC，容器拿到beanname和class类型，通过反射创建具体的对象，放入 concurrentHashmap中。 bean Definition，
    4. 代理模式
    5. 

19. 开放式问题，高可用、高性能、分布式、rpc、

    1. rpc
       1. rpc主要的优势，序列化/反序列化框架，没有过多的冗余数据，低成本就行通信。服务注册管理。序列化协议包含: 如基于文本编码的 xml json，也有二进制编码的 protobuf hessian等。
       2. rpc解决的问题：让分布式和微服务系统中不同的服务之间的调用像本地一样简单。
       3. 常见的rpc框架
          1. RMI，JDK自带的rpc框架
          2. Dubbo，阿里巴巴开源的高性能实现服务之间的输出与输出框架，基于spring框架
          3. Thrift，跨语言性能和出色的性能，
       4. 为什么不使用http，主要是传输协议和序列化协议，适用的场景不同，为微服务而生。而http是通用的应用层，超文本传输协议，比较冗余。一个POST协议的格式有很多无用的信息，可能一个消息有70%都是无用的消息，比如内容类型、长度、响应信息等。
          自定义的tcp协议可以大大精简。减少网络开销。
       5. 接口参数： 接口名次、参数类型、方法名、参数值、超时时间。
       6. RPC的Stub，从别的地方的流取出对象，或者把对象序列化成流，再发给目标机器。实现过一个原生的方案实现RPC框架，Stub这边使用Socket通信、动态代理与反射与Java原生的序列化。
       7. RPC架构分为三个部分
          1. 服务提供者，运行在服务端，提供服务接口定义与服务实现类
          2. 服务中心，运行在服务器端，负责将本地服务发布成远程服务，管理远程服务，提供给服务消费者使用
          3. 服务消费者，运行在客户端，通过远程代理调用远程服务
    2. 负载均衡
       1. 分库分表
       2. DNS轮询，北极星，通过发布机制将内容同步到大量的缓存节点。在dns服务器上找到离用户最近的节点提供服务，通常使用云。
       3. IP负载均衡。基于特定的TCP/IP技术实现的负载均衡，比如NAT、Turning等，使用的软件有nginx。nginx配置文件，绑定代理IP地址。
       4. 反向代理：服务器。正向代理：代理的是客户端

    

    

20. 原来的项目，一个请求的过程，spring视角、计算机网络视角
    spring：首先要组装一个请求体，路由到指定的机器的服务，根据url路径，通过一个dispatch定位到指定的controller的接口，然后调用相应的service服务，执行一定的逻辑的时候，根据需要再到持久层进行增删改查操作。执行完了之后再返回一个信息组装成一个返回体给前端展示给用户。
    1. DispatcherServlet——>
    2. HandlerMapping——> HandlerMapping将会把请求映射为HandlerExecutionChain对象 HandlerAdapter将会把处理器包装为适配器，从而支持多种类型的处理器
    3. HandlerAdapter——>处理器功能处理方法的调用，HandlerAdapter将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView对象（包含模型数据、逻辑视图名）；
    4. ModelAndView——>View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构
    5. ViewResolver——>
    6. View——>
    7. DispatcherServlet——>用户
    
26. 计算机网络视角，一次URL：应用层准备好应用数据，与服务器开启一次会话，这个期间要把应用数据经过加密等操作，包装成tcp支持的单元，就是加请求头组装成一个一个的TCP报文与服务器建立连接，传输层建立连接需要经历三次握手，根据序号和ack确认机制来保证数据传输的安全，同时在网络情况不好的时候也会加一些方法来进行拥塞控制，在与服务器交互的过程中，一个个的报文分成了更小的单元，就是网络层的概念，ip数据报是比较小的单位，1500字节一个，根据ip地址和相应的路由协议来找到合适的网关到达目的的网络。到了目的网络的时候，还需要找到对应的主机，根据ARP协议找到主机的mac地址，链路之间传输会分解成一个个的数据帧，再细说就是在网线里面分频或者分道编码解码之类的到目标机器里面。

27. 为什么不继续在腾讯实习
    
28. 自己业务的思考，数据链路：构建->追查case->写一些需求->改造读写方式->迁移。整个召回排序的架构，文档
    看以后分在什么岗位，是偏业务的还是偏基础架构的，

29. 对面试官有什么问题，
    业务大概是什么、我进去的技术栈是什么、进去之后表现好有转正机会吗